import tensorflow as tf
import numpy as np
from tensorflow.examples.tutorials.mnist import input_data

import matplotlib.pyplot as plt
import random

mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

nb_classes = 10

X = tf.placeholder(tf.float32, [None, 784])
Y = tf.placeholder(tf.float32, [None, nb_classes])

W = tf.Variable(tf.random_normal([784, nb_classes]))
b = tf.Variable(tf.random_normal([nb_classes]))

hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)
cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))

training_epochs = 15                     
batch_size = 100

with tf.Session() as sess:
    
    sess.run(tf.global_variables_initializer())
    
    for epoch in range(training_epochs):                      #epoch(전체)을 15번반복 (0~14)
        avg_cost = 0                        #각 epoch마다 avg_cost값초기화!!!
        total_batch = int(mnist.train.num_examples / batch_size)
        
        for i in range(total_batch):                          #bactch size를 iteration번 반복
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})
            
            avg_cost += c / batch_size    #C의 합/ batchsize
        
        print("Epoch: ","%04d" % (epoch + 1), "cost = ", "{:.1f}".format(avg_cost) )#천의자리수까지, 소수9자리까지
    
    print("Accuracy: ", accuracy.eval(session=sess,                 #accuracy.eval은 sess.run이랑같음. 단지accuracy 구할때 사용.session=sess 이건 안해줘도됨
                        feed_dict={X: mnist.test.images, Y: mnist.test.labels}))
    
    
    r = random.randint(0, mnist.test.num_examples - 1)  #1~10을  0~9로 가져오기!
    print("Label: ", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))  #one-hot을 argmax를 통해 숫자로 바꿔줌!
    print("Prediction: ", sess.run(tf.argmax(hypothesis, 1),
                              feed_dict={X: mnist.test.images[r:r + 1]}))

    plt.imshow(mnist.test.images[r:r+1].
              reshape(28,28), cmap='Greys', interpolation='nearest')
    plt.show
    
    ========================================================================
import tensorflow as tf
import numpy as np

from sklearn.preprocessing import MinMaxScaler




xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],
               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],
               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],
               [816, 820.958984, 1008100, 815.48999, 819.23999],
               [819.359985, 823, 1188100, 818.469971, 818.97998],
               [819, 823, 1198100, 816, 820.450012],
               [811.700012, 815.25, 1098100, 809.780029, 813.669983],
               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])

xy = MinMaxScaler(xy)             #오류뜸... 너무큰 array값들을 비율에맞게 0~1사이로 줄여줌.

x_data = xy[:, 0:-1]
y_data = xy[:, [-1]]

X = tf.placeholder(tf.float32, shape=[None, 4])
Y = tf.placeholder(tf.float32, shape=[None, 1])
W = tf.Variable(tf.random_normal([4,1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

hypothesis = tf.matmul(X, W) + b
cost = tf.reduce_mean(tf.square(hypothesis - Y))

optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)
train = optimizer.minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
for step in range(2001):
    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})
    
    print(step, cost_val, hy_val)
